{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5addd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### variaveis e Importações ####\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import urllib3\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "import gc\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) #### Desabilitar avisos de segurança do requests\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "\n",
    "#### Criar pasta de imagens, se não existir\n",
    "image_folder = \"images\"\n",
    "os.makedirs(image_folder, exist_ok=True)\n",
    "#### Nome do arquivo Excel\n",
    "excel_file = \"cards_data.xlsx\"\n",
    " \n",
    " #### Colunas do DataFrame\n",
    "columns = [\"Nome\", \"Cost\", \"Total Cost\", \"Type\", \"P\", \"T\", \"Printings\", \"Variações\", #### parte que vem do site compacto\n",
    "          \"Card Text:\", \"Flavor Text:\", \"Rarity:\", \"Artist:\", \"Link:\", \"Image Path\", \"Image URL\", \"Image Name\", \"id\", #### parte que vem da pagina da carta\n",
    "          \"Nome_A\", \"Cost_A\", \"Total Cost_A\", \"Type_A\", \"Card Text_A\", \"Card Number\", \"Expansion\"] #### parte que vem das cartas com 2 funções\n",
    "\n",
    "#### Carregar IDs existentes, se o arquivo já existir\n",
    "existing_ids = {}\n",
    "if os.path.exists(excel_file):\n",
    "  df_existing = pd.read_excel(excel_file, usecols=[\"id\"])\n",
    "  existing_ids = df_existing[\"id\"].fillna(0).astype(int).tolist()  #### Converter para lista de inteiros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d92df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### def salvar_imagem(soup, headers): ####\n",
    "\n",
    "def salvar_imagem(soup, headers):\n",
    "  # global img_path, img_url, img_name, numero\n",
    "  #### Criar pasta de imagens, se não existir\n",
    "  image_folder = \"images\"\n",
    "  os.makedirs(image_folder, exist_ok=True)\n",
    "\n",
    "  card_image_div = soup.find(\"div\", class_=\"cardImage\")\n",
    "  card_image_div_castOff = soup.find(\"table\", class_=\"cardDetails cardComponent\")\n",
    "\n",
    "  if card_image_div:\n",
    "    img_tag = card_image_div.find(\"img\")\n",
    "  elif card_image_div_castOff:\n",
    "    img_tag = card_image_div_castOff.find(\"img\")\n",
    "\n",
    "\n",
    "  if img_tag:\n",
    "    img_url = img_tag[\"src\"]\n",
    "    img_url = img_url.replace(\"../../\", \"https://gatherer.wizards.com/\") \n",
    "    img_name = img_tag[\"alt\"].replace(\" \", \"_\").replace(\"//\",\"&\").replace(\"\\\"\",\"&\").replace(\": \",\"-\") + \"-\" + setName + \"-\" + numero + \".jpg\"\n",
    "    img_path = os.path.join(image_folder, img_name)\n",
    "\n",
    "    #### Baixar e salvar a imagem\n",
    "    img_response = requests.get(img_url, headers=headers, verify=False, stream=True)\n",
    "    if img_response.status_code == 200:\n",
    "      with open(img_path, \"wb\") as img_file:\n",
    "        for chunk in img_response.iter_content(chunk_size=8192):  # Baixa e escreve em pedaços de 8KB\n",
    "          img_file.write(chunk)\n",
    "    else:\n",
    "      print(\"Erro ao baixar a imagem.\")\n",
    "      \n",
    "  dados_imagem = {\"Image Path\": img_path, \"Image URL\": img_url, \"Image Name\": img_name, \"Number\": numero} \n",
    "  return dados_imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc256848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### def obter_dados_cards(setUrl, Variation): #### return extracted_data, extracted_data2\n",
    "def obter_dados_cards(setUrl, Variation):\n",
    "  session2 = None  # Inicializa a variável\n",
    "  session3 = None  # Inicializa a variável\n",
    "\n",
    "  #### URL do card\n",
    "  url = f\"{setUrl}\"\n",
    "  data = [\"\"] * 14  #### Inicializa a lista com 8 elementos vazios\n",
    "  extracted_data = {}\n",
    "  extracted_data2 = {}\n",
    "  image_data = {}\n",
    "\n",
    "  #### Fazer requisição HTTP\n",
    "  session2 = requests.Session()\n",
    "  session3 = None\n",
    "\n",
    "  #### Acessa a pagina do card\n",
    "  response = session2.get(url, headers=HEADERS, verify=False)\n",
    "  if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    #### Encontrar variações do card\n",
    "    card_variations_div = soup.find(\"div\", class_=\"variations\")\n",
    "    if card_variations_div:\n",
    "      variations = card_variations_div.find_all(\"a\")   \n",
    "      if len(variations) > 1:           \n",
    "        variation_url = variations[Variation - 1][\"href\"]\n",
    "        variation_url = variation_url.replace(\"/Pages\", \"https://gatherer.wizards.com/Pages\")\n",
    "        url = variation_url\n",
    "\n",
    "        #### Fazer requisição HTTP para a variação\n",
    "        session3 = requests.Session()\n",
    "        response = session3.get(url, headers=HEADERS, verify=False)\n",
    "\n",
    "    session2.close()\n",
    "\n",
    "  #### Se existe mais de uma variação, verifica qual a pagina da variação 1, 2 etc na sequencia\n",
    "  #### Pois as vezes a variação 1 não é a 1ª na ordem alfabetica\n",
    "  if response.status_code == 200:\n",
    "    #### Para pegar o id da Carta\n",
    "    match = re.search(r\"multiverseid=(\\d+)\", url)\n",
    "    numero = match.group(1)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    image_data = salvar_imagem(soup, HEADERS)\n",
    "    \n",
    "    #### Sem CastOff\n",
    "    table_div = soup.find(\"div\", class_=\"smallGreyMono\", style=\"margin-top: 5px;\")\n",
    "    if table_div:\n",
    "      rows = table_div.find_all(\"div\", class_=\"label\")\n",
    "      values = table_div.find_all(\"div\", class_=\"value\")\n",
    "      extracted_data = {row.text.strip(): value.text.strip() \n",
    "                        for row, value in zip(rows, values)}\n",
    "\n",
    "\n",
    "    #### Com CastOff\n",
    "    table_div_cast = soup.find_all(\"div\", class_=\"smallGreyMono\", style=None)\n",
    "    counterA = 0\n",
    "    for table_div in table_div_cast:\n",
    "      # table_div = table_div_cast[0]\n",
    "      if table_div and counterA == 0:\n",
    "        rows = table_div.find_all(\"div\", class_=\"label\")\n",
    "        values = table_div.find_all(\"div\", class_=\"value\")\n",
    "        extracted_data = {row.text.strip(): value.text.strip() \n",
    "                          for row, value in zip(rows, values)}\n",
    "\n",
    "      if table_div and counterA == 1:\n",
    "        rows = table_div.find_all(\"div\", class_=\"label\")\n",
    "        values = table_div.find_all(\"div\", class_=\"value\")\n",
    "        extracted_data2 = {row.text.strip(): value.text.strip() \n",
    "                           for row, value in zip(rows, values)}\n",
    "\n",
    "      print(f\"extracted_data: {extracted_data}\")\n",
    "      print(f\"extracted_data2: {extracted_data2}\")    \n",
    "      print(f\"image_data: {image_data}\")\n",
    "      counterA += 1\n",
    "\n",
    "\n",
    "  else:\n",
    "    print(\"Erro ao acessar a página.\")\n",
    "\n",
    "  if session2 == None:\n",
    "    session3.close()\n",
    "  else:\n",
    "    session2.close()\n",
    "    extracted_data2 = extracted_data2.append(extracted_data, ignore_index=True)\n",
    "  return   extracted_data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d255d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### def salvar_em_excel(lista_cards): ####\n",
    "def salvar_em_excel(extracted_data):\n",
    "\n",
    "  df_novo = pd.DataFrame([extracted_data])\n",
    "\n",
    "  try:\n",
    "    # Adicionando ao arquivo Excel existente\n",
    "    with pd.ExcelWriter(excel_file, mode=\"a\", if_sheet_exists=\"overlay\", engine=\"openpyxl\") as writer:\n",
    "      df_novo.to_excel(writer, index=False, header=False)\n",
    "\n",
    "  except Exception as e:\n",
    "            print(f\"Erro inesperado na linha {index}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3ee1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### def dados_da_pagina_principal(soup, existing_ids): ####\n",
    "def dados_da_pagina_principal(soup, existing_ids):\n",
    "  lista_dados = []  # Lista para armazenar os dicionários\n",
    "\n",
    "  for row in soup.find_all(\"tr\"):\n",
    "    cols = row.find_all(\"td\")\n",
    "      \n",
    "    if not cols:\n",
    "      continue  # Evita erros com linhas sem colunas\n",
    "    \n",
    "    name = cols[0].text.strip()\n",
    "    if name in [\"Name\", None]: \n",
    "      continue\n",
    "    \n",
    "    if int(numero) in existing_ids:\n",
    "      print(\"p\", end=\"\") # imprimi p se o item ja existe no excel\n",
    "      continue\n",
    "\n",
    "    link_tag = cols[0].find(\"a\")\n",
    "    link = link_tag[\"href\"] if link_tag else \"\"\n",
    "    link = link.replace(\"../\", \"https://gatherer.wizards.com/Pages/\")\n",
    "    \n",
    "    match = re.search(r\"multiverseid=(\\d+)\", link)\n",
    "    numero = match.group(1) if match else None\n",
    "\n",
    "    qtdImages = len(cols[5].find_all(\"img\"))\n",
    "\n",
    "    extracted_data = {\n",
    "      \"Nome\": name,\n",
    "      \"Cost\": \", \".join(img[\"alt\"] for img in cols[1].find_all(\"img\")),\n",
    "      \"Total Cost\": len(cols[1].find_all(\"img\")),\n",
    "      \"Type\": cols[2].text.strip(),\n",
    "      \"P\": cols[3].text.strip(),\n",
    "      \"T\": cols[4].text.strip(),\n",
    "      \"Printings\": \", \".join(img[\"alt\"] for img in cols[5].find_all(\"img\"))\n",
    "    }\n",
    "  \n",
    "    lista_dados.append(extracted_data)  # Adiciona o dicionário à lista\n",
    "\n",
    "\n",
    "    for i in range(0, qtdImages ):\n",
    "      print(i, end=\"\")\n",
    "      card= []\n",
    "      extracted_data2 = obter_dados_cards(link, i)\n",
    "      lista_dados.append(extracted_data2)\n",
    "\n",
    "  return lista_dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbcb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### def processar_cada_linha(existing_ids, soup): ####\n",
    "def processar_cada_linha(existing_ids, soup):\n",
    "\n",
    "  lista_dados = []  # Lista para armazenar os dicionários\n",
    "\n",
    "  for row in soup.find_all(\"tr\"):\n",
    "    cols = row.find_all(\"td\")\n",
    "      \n",
    "    if not cols:\n",
    "      continue  # Evita erros com linhas sem colunas\n",
    "    \n",
    "    name = cols[0].text.strip()\n",
    "    if name in [\"Name\", None]: \n",
    "      continue\n",
    "\n",
    "    link_tag = cols[0].find(\"a\")\n",
    "    link = link_tag[\"href\"] if link_tag else \"\"\n",
    "    link = link.replace(\"../\", \"https://gatherer.wizards.com/Pages/\")\n",
    "    \n",
    "    match = re.search(r\"multiverseid=(\\d+)\", link)\n",
    "    numero = match.group(1) if match else None\n",
    "    \n",
    "    if int(numero) in existing_ids:\n",
    "      print(\"p\", end=\"\") # imprimi p se o item ja existe no excel\n",
    "      continue\n",
    "\n",
    "    extracted_data = {\n",
    "      \"Nome\": name,\n",
    "      \"Cost\": \", \".join(img[\"alt\"] for img in cols[1].find_all(\"img\")),\n",
    "      \"Total Cost\": len(cols[1].find_all(\"img\")),\n",
    "      \"Type\": cols[2].text.strip(),\n",
    "      \"P\": cols[3].text.strip(),\n",
    "      \"T\": cols[4].text.strip(),\n",
    "      \"Printings\": \", \".join(img[\"alt\"] for img in cols[5].find_all(\"img\"))\n",
    "    }\n",
    "    \n",
    "    lista_dados.append(extracted_data)  # Adiciona o dicionário à lista\n",
    "    print(f\"Principal: {lista_dados}\")\n",
    "\n",
    "    qtdImages = len(cols[5].find_all(\"img\")) # Total de variações\n",
    "    for i in range(0, qtdImages ):\n",
    "      print(i, end=\"\")\n",
    "      \n",
    "      #### pega dados da variação, se houver\n",
    "      extracted_data2 = obter_dados_cards(link, i)\n",
    "      lista_dados.append(extracted_data2)\n",
    "      print(f\"Variação {i}: {lista_dados}\")\n",
    "\n",
    "      #### salva no Excel\n",
    "      salvar_em_excel(lista_dados)\n",
    "      \n",
    "      # adicionar o id à lista de ids existentes\n",
    "      try:\n",
    "        existing_ids.append(int(numero))\n",
    "      except:\n",
    "        print(f\"Erro ao converter o ID {numero} para inteiro.\")        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60fcf280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### def processar(setName): ####\n",
    "def processar(setName):\n",
    "  #### Carrega a página com a lista de cards\n",
    "  page = 0\n",
    "  formatted_setName = setName.replace(\" \", \"+\")  #### Substituir espaços por '+'\n",
    "  url = f\"https://gatherer.wizards.com/Pages/Search/Default.aspx?sort=name+&page={page}&output=compact&set=[%22{formatted_setName}%22]\"\n",
    "\n",
    "  #### Fazer requisição para a página\n",
    "  session0 = requests.Session()\n",
    "  response = session0.get(url, verify=False)\n",
    "\n",
    "  #### Se a resposta não for válida, interrompe o loop\n",
    "  if response.status_code == 200:  \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    #### Encontrar a div com a classe \"pagincontrols\"\n",
    "    pagin_controls = soup.find(\"div\", class_=\"pagingcontrols\")\n",
    "    if pagin_controls:\n",
    "      links = pagin_controls.find_all(\"a\")\n",
    "      qtdPages = len(links)  #### -1 para não contar o link \"Next\" e começar com 0\n",
    "\n",
    "      if qtdPages <= 1: \n",
    "        qtdPages = 1  \n",
    "      else:\n",
    "        qtdPages = qtdPages - 1\n",
    "    session0.close()\n",
    "\n",
    "  #### Ajuste o número de páginas conforme necessário\n",
    "  for page in range(0, qtdPages):\n",
    "\n",
    "    # Carrega a pagina com links para cada versão do card\n",
    "    url = f\"https://gatherer.wizards.com/Pages/Search/Default.aspx?sort=name+&page={page}&output=compact&set=[%22{formatted_setName}%22]\"\n",
    "\n",
    "    #### Refazer requisição para a página\n",
    "    session1 = requests.Session()\n",
    "    response = session1.get(url, verify=False)\n",
    "\n",
    "    #### Adicionar um pequeno atraso entre as requisições      \n",
    "    time.sleep(2)  \n",
    "\n",
    "    #### Se a resposta não for válida, interrompe o loop\n",
    "    if response.status_code == 200:  \n",
    "      soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "      contador = 0\n",
    "\n",
    "      print(f\"\\n    {setName} - página {page + 1} de {qtdPages}\")\n",
    "      print(\"123456789.123456789.123456789.123456789.123456789.123456789.123456789.123456789.123456789.1234567890123456789.123456789.123456789.123456789.123456789.\")\n",
    "\n",
    "      #### Lê cada linha da pagina no formato compacto      \n",
    "      processar_cada_linha(existing_ids, soup)\n",
    "\n",
    "    session1.close()  #### Fechar a sessão do requests\n",
    "    gc.collect()\n",
    "\n",
    "    # Reiniciar Python após processar um set\n",
    "    print(\"\\nReiniciando o Python\")\n",
    "    quit()\n",
    "    os.system(\"MTG v2.ipynb\")\n",
    "  # fim do for page in range(0, qtdPages):\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquivo contendo a lista original de sets\n",
    "sets_file = \"sets.txt\"\n",
    "processed_file = \"processed_sets.txt\" # Arquivo para armazenar os sets já processados\n",
    "with open(sets_file, \"r\", encoding=\"utf-8\") as f: # Ler lista original e os já processados\n",
    "    sets_list = [line.strip() for line in f.readlines()]\n",
    "if os.path.exists(processed_file):\n",
    "    with open(processed_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        processed_list = {line.strip() for line in f.readlines()}\n",
    "else:\n",
    "    processed_list = set()\n",
    "# Filtrar sets que ainda não foram processados\n",
    "remaining_sets = [set_name for set_name in sets_list if set_name not in processed_list]\n",
    "setName = remaining_sets[0] if remaining_sets else None\n",
    "\n",
    "# Se houver sets restantes, processar o primeiro e salvar no arquivo\n",
    "if remaining_sets:\n",
    "    processar(setName)\n",
    "  \n",
    "    # Registrar o set como processado\n",
    "    with open(processed_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(setName + \"\\n\")\n",
    "\n",
    "    # Reiniciar Python após processar um set\n",
    "    # print(\"\\nReiniciando o Python\")\n",
    "    # exit()\n",
    "    # os.system(\"MTG v2.ipynb\")\n",
    "else:\n",
    "  print(\"Todos os sets foram processados!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
